global:
  resolve_timeout: 5m
  slack_api_url: '${SLACK_API_URL}'
  opsgenie_api_url: '${OPSGENIE_API_URL}'

route:
  group_by: ['alertname', 'region', 'type']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: 'default'
  routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
      repeat_interval: 1h
      continue: true
    - match:
        type: consensus
      receiver: 'consensus-team'
      continue: true
    - match:
        type: performance
      receiver: 'ops-team'
    - match:
        type: security
      receiver: 'security-team'
      continue: true

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['region', 'type']

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#besu-alerts'
        send_resolved: true
        title: '{{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        footer: 'Alertmanager'

  - name: 'critical-alerts'
    opsgenie_configs:
      - api_key: '${OPSGENIE_API_KEY}'
        priority: P1
        message: '{{ template "opsgenie.message" . }}'
        description: '{{ template "opsgenie.description" . }}'
        tags: ['besu', '{{ .GroupLabels.type }}', '{{ .GroupLabels.region }}']
    slack_configs:
      - channel: '#besu-critical'
        send_resolved: true
        title: '[CRITICAL] {{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        footer: 'Alertmanager - Critical Alert'

  - name: 'consensus-team'
    slack_configs:
      - channel: '#besu-consensus'
        send_resolved: true
        title: '[CONSENSUS] {{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        footer: 'Alertmanager - Consensus Team'
    email_configs:
      - to: 'consensus-team@company.com'
        send_resolved: true
        headers:
          subject: '[CONSENSUS] {{ template "email.subject" . }}'

  - name: 'ops-team'
    slack_configs:
      - channel: '#besu-ops'
        send_resolved: true
        title: '[OPS] {{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        footer: 'Alertmanager - Ops Team'
    email_configs:
      - to: 'ops-team@company.com'
        send_resolved: true
        headers:
          subject: '[OPS] {{ template "email.subject" . }}'

  - name: 'security-team'
    slack_configs:
      - channel: '#besu-security'
        send_resolved: true
        title: '[SECURITY] {{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        footer: 'Alertmanager - Security Team'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        description: '{{ template "pagerduty.description" . }}'
        severity: '{{ if eq .GroupLabels.severity "critical" }}critical{{ else }}warning{{ end }}'
        client: 'Besu Network Monitoring'
        client_url: '{{ template "pagerduty.client.url" . }}'

templates:
  - '/etc/alertmanager/templates/*.tmpl'

groups:
- name: besu-network-alerts
  rules:
  - alert: BesuNodeDown
    expr: up{job="besu-nodes"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Besu node is down"
      description: "Node {{ $labels.instance }} has been down for more than 5 minutes"

  - alert: LowPeerCount
    expr: besu_peers < 3
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Low peer count"
      description: "Node {{ $labels.instance }} has less than 3 peers"

  - alert: BlockchainNotSyncing
    expr: rate(besu_blockchain_height[15m]) == 0
    for: 15m
    labels:
      severity: critical
    annotations:
      summary: "Blockchain not syncing"
      description: "Node {{ $labels.instance }} blockchain height has not increased in 15 minutes"

  - alert: HighTransactionPoolSize
    expr: besu_transaction_pool_transactions > 1000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High transaction pool size"
      description: "Node {{ $labels.instance }} transaction pool size is high"